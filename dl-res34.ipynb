{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93057,"databundleVersionId":11145869,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# List files in the working directory\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"execution":{"iopub.status.busy":"2025-03-12T05:15:28.775070Z","iopub.execute_input":"2025-03-12T05:15:28.775317Z","iopub.status.idle":"2025-03-12T05:15:28.780571Z","shell.execute_reply.started":"2025-03-12T05:15:28.775293Z","shell.execute_reply":"2025-03-12T05:15:28.779688Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['.virtual_documents']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport pickle\nimport numpy as np\nimport copy","metadata":{"execution":{"iopub.status.busy":"2025-03-12T05:15:28.781419Z","iopub.execute_input":"2025-03-12T05:15:28.781744Z","iopub.status.idle":"2025-03-12T05:15:35.107972Z","shell.execute_reply.started":"2025-03-12T05:15:28.781709Z","shell.execute_reply":"2025-03-12T05:15:35.107312Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"###############################################################################\n# 1. Data Preparation: CIFAR-10 + Cutout\n###############################################################################\nclass Cutout:\n    \"\"\"\n    Randomly masks out square regions of an image. Assumes the input is a tensor.\n    \"\"\"\n    def __init__(self, n_holes=1, length=16):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        \"\"\"\n        img: tensor image of size (C, H, W)\n        \"\"\"\n        h = img.size(1)\n        w = img.size(2)\n\n        # Create a mask full of 1s\n        mask = np.ones((h, w), np.float32)\n\n        for _ in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask).to(img.device)\n        mask = mask.expand_as(img)\n        img = img * mask\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + f'(n_holes={self.n_holes}, length={self.length})'\n\n\ndef get_cifar10_dataloaders(batch_size=128, num_workers=2):\n    mean = (0.4914, 0.4822, 0.4465)\n    std = (0.2470, 0.2435, 0.2616)\n\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n        transforms.ToTensor(),\n        Cutout(n_holes=1, length=16),\n        transforms.Normalize(mean, std),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    train_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n    )\n    return train_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2025-03-12T05:15:35.109572Z","iopub.execute_input":"2025-03-12T05:15:35.110162Z","iopub.status.idle":"2025-03-12T05:15:35.118922Z","shell.execute_reply.started":"2025-03-12T05:15:35.110136Z","shell.execute_reply":"2025-03-12T05:15:35.118102Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"###############################################################################\n# 2. Narrow ResNet-34 Implementation (same as before)\n###############################################################################\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding, no bias.\"\"\"\n    return nn.Conv2d(\n        in_planes, out_planes, kernel_size=3, stride=stride,\n        padding=1, bias=False\n    )\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None, bn_momentum=0.9):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass NarrowResNet34(nn.Module):\n    def __init__(self, num_classes=10, width_multiplier=0.75,\n                 bn_momentum=0.9, dropout=0.0):\n        \"\"\"\n        ResNet-34 with fewer channels (width_multiplier) and optional dropout.\n        \"\"\"\n        super(NarrowResNet34, self).__init__()\n        self.block_layers = [3, 4, 6, 3]\n        base_channels = [64, 128, 256, 512]\n        self.channels = [int(c * width_multiplier) for c in base_channels]\n        self.in_planes = self.channels[0]\n\n        # Initial convolution\n        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_planes, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Stages\n        self.layer1 = self._make_layer(self.channels[0], self.block_layers[0],\n                                       stride=1, bn_momentum=bn_momentum)\n        self.layer2 = self._make_layer(self.channels[1], self.block_layers[1],\n                                       stride=2, bn_momentum=bn_momentum)\n        self.layer3 = self._make_layer(self.channels[2], self.block_layers[2],\n                                       stride=2, bn_momentum=bn_momentum)\n        self.layer4 = self._make_layer(self.channels[3], self.block_layers[3],\n                                       stride=2, bn_momentum=bn_momentum)\n\n        # Global average pooling + dropout + linear\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=dropout)\n        self.fc = nn.Linear(self.channels[3] * BasicBlock.expansion, num_classes)\n\n        self._init_weights()\n\n    def _make_layer(self, planes, blocks, stride=1, bn_momentum=0.9):\n        downsample = None\n        if stride != 1 or self.in_planes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride,\n                          bias=False),\n                nn.BatchNorm2d(planes, momentum=bn_momentum),\n            )\n\n        layers = []\n        layers.append(BasicBlock(self.in_planes, planes, stride, downsample,\n                                 bn_momentum=bn_momentum))\n        self.in_planes = planes\n        for _ in range(1, blocks):\n            layers.append(BasicBlock(self.in_planes, planes, bn_momentum=bn_momentum))\n        return nn.Sequential(*layers)\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2025-03-12T05:15:35.120269Z","iopub.execute_input":"2025-03-12T05:15:35.120571Z","iopub.status.idle":"2025-03-12T05:15:35.145510Z","shell.execute_reply.started":"2025-03-12T05:15:35.120542Z","shell.execute_reply":"2025-03-12T05:15:35.144864Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"###############################################################################\n# 3. Training and Evaluation Routines\n###############################################################################\ndef train_one_epoch(model, device, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        total += targets.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = 100.0 * correct / total\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model, device, test_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(targets).sum().item()\n            total += targets.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = 100.0 * correct / total\n    return epoch_loss, epoch_acc\n","metadata":{"execution":{"iopub.status.busy":"2025-03-12T05:15:35.146294Z","iopub.execute_input":"2025-03-12T05:15:35.146558Z","iopub.status.idle":"2025-03-12T05:15:35.165531Z","shell.execute_reply.started":"2025-03-12T05:15:35.146534Z","shell.execute_reply":"2025-03-12T05:15:35.164912Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"###############################################################################\n# 4. Structured Pruning Utilities\n#    - We'll gather channel importance (L1 norm) for each conv layer's output\n#      channels, then keep top-K channels (1 - prune_ratio).\n#    - Then we build a physically smaller model with fewer channels.\n###############################################################################\ndef get_conv_layers(model):\n    \"\"\"\n    Return a list of (module, downsample_module_if_any) for each residual block's conv layers.\n    We'll also get the matching downsample if it exists, so we can prune it consistently.\n    \"\"\"\n    conv_layers = []\n\n    def check_downsample_for_conv(ds):\n        # If downsample is None or doesn't have a conv2d, return None\n        if ds is None:\n            return None\n        for m in ds.modules():\n            if isinstance(m, nn.Conv2d):\n                return m\n        return None\n\n    # We'll do a recursive search. Alternatively, we can just check model.layer1, layer2, ...\n    # but let's do a helper function:\n\n    def find_convs_in_sequential(seq):\n        for block in seq:\n            if isinstance(block, BasicBlock):\n                # block.conv1, block.conv2, possibly block.downsample\n                conv_layers.append((block.conv1, check_downsample_for_conv(block.downsample)))\n                conv_layers.append((block.conv2, None))  # conv2 typically doesn't have a new downsample\n            else:\n                # If there's nested structure, we'd check recursively\n                pass\n\n    find_convs_in_sequential(model.layer1)\n    find_convs_in_sequential(model.layer2)\n    find_convs_in_sequential(model.layer3)\n    find_convs_in_sequential(model.layer4)\n\n    return conv_layers\n\ndef compute_channel_importance(model, device):\n    \"\"\"\n    For each Conv2d layer, compute the L1 norm of each output channel's weights.\n    We'll store them in a list with the shape: layer_importance[i] = (conv, [c0_norm, c1_norm, ...]).\n    \"\"\"\n    convs = get_conv_layers(model)\n    layer_importance = []\n\n    for (conv, ds_conv) in convs:\n        w = conv.weight.data.abs().sum(dim=(1,2,3))  # L1 norm per out_channel\n        layer_importance.append((conv, ds_conv, w.cpu().numpy()))\n    return layer_importance\n\ndef pick_channels_to_keep(importance, prune_ratio):\n    \"\"\"\n    Given a vector of channel importances, pick the top-K channels to keep based on L1 norm.\n    If prune_ratio=0.3, we keep 70% channels with the largest norms.\n    Returns an array of indices (sorted ascending) of the channels to keep.\n    \"\"\"\n    num_channels = len(importance)\n    k = int(num_channels * (1 - prune_ratio))\n    if k < 1:\n        k = 1\n    # Sort channels by descending importance\n    sorted_indices = np.argsort(-importance)  # descending\n    keep = sorted_indices[:k]\n    keep = np.sort(keep)  # for convenience\n    return keep\n\ndef build_pruned_conv(old_conv, keep_indices, in_channels_to_keep=None):\n    \"\"\"\n    Create a new nn.Conv2d with fewer out_channels (and optionally fewer in_channels),\n    and copy the relevant weights from `old_conv`.\n    - keep_indices: which output channels to keep\n    - in_channels_to_keep: which input channels to keep (None => keep all)\n    \"\"\"\n    old_weight = old_conv.weight.data\n    out_channels = len(keep_indices)\n\n    if in_channels_to_keep is None:\n        in_channels_to_keep = range(old_weight.shape[1])\n\n    new_conv = nn.Conv2d(\n        in_channels=len(in_channels_to_keep),\n        out_channels=out_channels,\n        kernel_size=old_conv.kernel_size,\n        stride=old_conv.stride,\n        padding=old_conv.padding,\n        dilation=old_conv.dilation,\n        groups=1,  # we won't handle grouped conv in this example\n        bias=(old_conv.bias is not None)\n    )\n\n    # Copy weights\n    with torch.no_grad():\n        for new_i, old_i in enumerate(keep_indices):\n            for new_j, old_j in enumerate(in_channels_to_keep):\n                new_conv.weight[new_i, new_j] = old_weight[old_i, old_j]\n\n        if old_conv.bias is not None:\n            old_bias = old_conv.bias.data\n            new_conv.bias.data = old_bias[keep_indices].clone()\n\n    return new_conv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:15:35.166489Z","iopub.execute_input":"2025-03-12T05:15:35.166799Z","iopub.status.idle":"2025-03-12T05:15:35.184110Z","shell.execute_reply.started":"2025-03-12T05:15:35.166770Z","shell.execute_reply":"2025-03-12T05:15:35.183487Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def prune_model_structured(model, prune_ratio=0.3, device='cuda'):\n    \"\"\"\n    1) Compute channel importance for each conv layer.\n    2) Decide which channels to keep in each layer.\n    3) Rebuild the entire model with fewer channels.\n    4) Copy weights.\n\n    Returns the pruned model.\n    \"\"\"\n    model = copy.deepcopy(model).to(device)  # copy to avoid in-place modification\n    model.eval()\n\n    layer_importance = compute_channel_importance(model, device)\n    # layer_importance is a list of (conv, ds_conv, w_array)\n\n    # We'll store for each layer:\n    # - which out_channels we keep\n    # Then we also need to track in_channels to keep for the *next* layer.\n\n    keep_out_channels = []\n    for (conv, ds_conv, w) in layer_importance:\n        out_keep = pick_channels_to_keep(w, prune_ratio)\n        keep_out_channels.append((conv, ds_conv, out_keep))\n\n    # We'll now build a new model with the pruned structure\n    # 1) Start from the existing model\n    pruned_model = NarrowResNet34(\n        num_classes=model.fc.out_features,\n        width_multiplier=1.0,   # We'll override channel counts manually\n        bn_momentum=0.9,\n        dropout=model.dropout.p if isinstance(model.dropout, nn.Dropout) else 0.0\n    ).to(device)\n\n    # Because we are manually setting channels, let's gather all conv references\n    old_convs = get_conv_layers(model)\n    new_convs = get_conv_layers(pruned_model)\n\n    # Also handle the very first conv1:\n    old_first_conv = model.conv1\n    new_first_conv = pruned_model.conv1\n\n    # We'll store the in_channels that go into each block's conv\n    prev_keep = range(old_first_conv.weight.shape[1])  # initially keep all RGB\n    # Now prune the first conv1 output channels\n    first_out_channels = old_first_conv.weight.shape[0]\n    w_first = old_first_conv.weight.data.abs().sum(dim=(1,2,3)).cpu().numpy()\n    keep_first = pick_channels_to_keep(w_first, prune_ratio)\n\n    # Rebuild conv1\n    pruned_conv1 = build_pruned_conv(old_first_conv, keep_first, in_channels_to_keep=prev_keep)\n    pruned_model.conv1 = pruned_conv1\n\n    # Now we update new_first_conv BN\n    old_bn1 = model.bn1\n    new_bn1 = pruned_model.bn1\n    with torch.no_grad():\n        new_bn1.weight.data = old_bn1.weight.data[keep_first].clone()\n        new_bn1.bias.data = old_bn1.bias.data[keep_first].clone()\n        new_bn1.running_mean = old_bn1.running_mean[keep_first].clone()\n        new_bn1.running_var = old_bn1.running_var[keep_first].clone()\n\n    current_in_channels = keep_first  # for the next block\n\n    # We apply the same logic for each residual block conv\n    # keep_out_channels is parallel to old_convs\n    for i, ((old_conv, old_ds_conv), (conv, ds_conv, out_keep)) in enumerate(zip(old_convs, keep_out_channels)):\n        (new_conv, new_ds_conv) = new_convs[i]\n\n        # Build the pruned conv for the main branch\n        pruned_conv = build_pruned_conv(old_conv, out_keep, in_channels_to_keep=current_in_channels)\n        # Replace it in the new model\n        # We have to do this carefully: \"old_conv\" is a reference inside a BasicBlock,\n        # so let's set the new block's conv weight.\n        new_conv.weight = nn.Parameter(pruned_conv.weight.data.clone())\n        if pruned_conv.bias is not None:\n            new_conv.bias = nn.Parameter(pruned_conv.bias.data.clone())\n\n        # Also, copy BN\n        old_bn = None\n        # We can find the BN by walking upward in the parent block. For simplicity:\n        # we assume the next line is the BN for old_conv.\n        # We'll do it by direct attribute search in the BasicBlock\n        old_block = find_parent_block(model, old_conv)\n        new_block = find_parent_block(pruned_model, new_conv)\n        if old_block is not None and new_block is not None:\n            if old_block.conv1 is old_conv:\n                old_bn = old_block.bn1\n                new_bn = new_block.bn1\n            elif old_block.conv2 is old_conv:\n                old_bn = old_block.bn2\n                new_bn = new_block.bn2\n\n            if old_bn is not None:\n                with torch.no_grad():\n                    new_bn.weight.data = old_bn.weight.data[out_keep].clone()\n                    new_bn.bias.data = old_bn.bias.data[out_keep].clone()\n                    new_bn.running_mean = old_bn.running_mean[out_keep].clone()\n                    new_bn.running_var = old_bn.running_var[out_keep].clone()\n\n        # If there's a downsample conv, we prune it the same way\n        if old_ds_conv is not None:\n            # The ds_conv might change both out_channels and in_channels.\n            # Typically, it changes out_channels to match old_conv.out_planes.\n            ds_out_keep = out_keep  # we want consistency\n            pruned_ds_conv = build_pruned_conv(old_ds_conv, ds_out_keep, in_channels_to_keep=current_in_channels)\n\n            # Now replace the new model's downsample conv\n            new_ds_block = find_downsample_block(pruned_model, new_block)\n            if new_ds_block is not None:\n                for m in new_ds_block.modules():\n                    if isinstance(m, nn.Conv2d):\n                        m.weight = nn.Parameter(pruned_ds_conv.weight.data.clone())\n                        if pruned_ds_conv.bias is not None:\n                            m.bias = nn.Parameter(pruned_ds_conv.bias.data.clone())\n\n                # Copy BN as well\n                old_ds_bn = find_downsample_bn(old_block)\n                new_ds_bn = find_downsample_bn(new_block)\n                if old_ds_bn is not None and new_ds_bn is not None:\n                    with torch.no_grad():\n                        new_ds_bn.weight.data = old_ds_bn.weight.data[ds_out_keep].clone()\n                        new_ds_bn.bias.data = old_ds_bn.bias.data[ds_out_keep].clone()\n                        new_ds_bn.running_mean = old_ds_bn.running_mean[ds_out_keep].clone()\n                        new_ds_bn.running_var = old_ds_bn.running_var[ds_out_keep].clone()\n\n            # The output channels of this block is the same \"out_keep\"\n            current_in_channels = out_keep\n        else:\n            # No downsample => the output shape is out_keep\n            current_in_channels = out_keep\n\n    # Finally, we need to adjust the final layer (pruned_model.fc) to match the last layer's out_channels\n    old_fc = model.fc\n    new_fc = pruned_model.fc\n    last_keep = current_in_channels  # channels from the final conv block\n    in_features = len(last_keep)\n    # Build a new fc\n    new_fc2 = nn.Linear(in_features, old_fc.out_features).to(device)\n    # Copy weights\n    with torch.no_grad():\n        for new_i, old_i in enumerate(last_keep):\n            new_fc2.weight[:, new_i] = old_fc.weight[:, old_i]\n        new_fc2.bias[:] = old_fc.bias[:]\n\n    pruned_model.fc = new_fc2\n\n    # Now we can check param count\n    return pruned_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:15:35.184837Z","iopub.execute_input":"2025-03-12T05:15:35.185040Z","iopub.status.idle":"2025-03-12T05:15:35.205298Z","shell.execute_reply.started":"2025-03-12T05:15:35.185022Z","shell.execute_reply":"2025-03-12T05:15:35.204746Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def find_parent_block(model, conv):\n    \"\"\"\n    Given a model and a conv layer, try to find the BasicBlock that owns it.\n    \"\"\"\n    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n        layer = getattr(model, layer_name)\n        for block in layer:\n            if isinstance(block, BasicBlock):\n                if block.conv1 is conv or block.conv2 is conv:\n                    return block\n    return None\n\ndef find_downsample_block(model, block):\n    \"\"\"\n    Return block.downsample if it exists. We can't do block.downsample directly\n    because we only have the 'new_block' from the pruned model. We'll just return block.downsample\n    if it exists. This function is used after we find the parent block in the pruned model.\n    \"\"\"\n    if hasattr(block, 'downsample'):\n        return block.downsample\n    return None\n\ndef find_downsample_bn(old_block):\n    if old_block is None or old_block.downsample is None:\n        return None\n    for m in old_block.downsample.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            return m\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:15:35.207390Z","iopub.execute_input":"2025-03-12T05:15:35.207638Z","iopub.status.idle":"2025-03-12T05:15:35.222862Z","shell.execute_reply.started":"2025-03-12T05:15:35.207612Z","shell.execute_reply":"2025-03-12T05:15:35.222066Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"###############################################################################\n# 4. Main Script: Training Loop with ResNet-34\n###############################################################################\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Using device:\", device)\n\n# Hyperparameters\nbatch_size = 128\ninitial_lr = 0.1\nweight_decay = 5e-4\nnum_epochs_preprune = 200  \nnum_epochs_finetune = 50  \n\n# 1) Load Data\ntrain_loader, test_loader = get_cifar10_dataloaders(batch_size=batch_size)\n\n# 2) Create Model and Train\nmodel = NarrowResNet34(\n        num_classes=10,\n        width_multiplier=0.75,   # yields >5M params if not pruned\n        bn_momentum=0.9,\n        dropout=0.5\n    ).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Unpruned model param count: {total_params:,}\")\n\n# Train for some epochs\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.SGD(\n        model.parameters(), lr=initial_lr, momentum=0.9,\n        weight_decay=weight_decay, nesterov=True\n    )\nlr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=180, eta_min=0.0\n)\n\n# Stage A: Train unpruned\nfor epoch in range(num_epochs_preprune):\n    train_loss, train_acc = train_one_epoch(model, device, train_loader, criterion, optimizer)\n    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n    lr_scheduler.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"[Preprune] Epoch {epoch+1}/{num_epochs_preprune} \"\n                f\"Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-03-12T05:15:35.223860Z","iopub.execute_input":"2025-03-12T05:15:35.224126Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 47.9M/170M [00:02<00:04, 24.6MB/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"    # 3) Structured Pruning\nprune_ratio = 0.5  # keep 40% of each layer's output channels\npruned_model = prune_model_structured(model, prune_ratio=prune_ratio, device=device)\npruned_model.to(device)\npruned_model.eval()\n\n# Check parameter count again\npruned_params = sum(p.numel() for p in pruned_model.parameters())\nprint(f\"Pruned model param count: {pruned_params:,}\")\n\n# 4) Fine-tune pruned model\n# typically you do a smaller LR\nft_lr = 0.01\nft_optimizer = optim.SGD(\n    pruned_model.parameters(), lr=ft_lr, momentum=0.9,\n    weight_decay=weight_decay, nesterov=True\n)\nft_scheduler = optim.lr_scheduler.StepLR(ft_optimizer, step_size=5, gamma=0.1)\n\nfor epoch in range(num_epochs_finetune):\n    train_loss, train_acc = train_one_epoch(pruned_model, device, train_loader, criterion, ft_optimizer)\n    test_loss, test_acc = evaluate(pruned_model, device, test_loader, criterion)\n    ft_scheduler.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"[Finetune] Epoch {epoch+1}/{num_epochs_finetune} \"\n                f\"Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n\n    # Final Test\nfinal_test_loss, final_test_acc = evaluate(pruned_model, device, test_loader, criterion)\nprint(f\"\\nFinal pruned model test accuracy: {final_test_acc:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model weights\ntorch.save(model, \"/kaggle/working/resnet18_v1.pth\")\nprint(\"Model saved to resnet18_v1.pth\")\n\ntorch.save(model.state_dict(), \"/kaggle/working/resnet18_v1_state.pth\")\nprint(\"Model saved to resnet18_v1_state.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pickle\n\n#import torch\nimport pandas as pd\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n########################################################################\n# 1) Set up Device\n########################################################################\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Using device:\", device)\n\n########################################################################\n# 2) Load and Convert the Test File\n########################################################################\ndef load_custom_test_set(filepath):\n    # Open the pickle file and load the data\n    with open(filepath, 'rb') as f:\n        data = pickle.load(f, encoding='bytes')\n    \n    # Extract image data\n    images = data[b'data']  # Shape: (10000, 32, 32, 3)\n    \n    # ✅ Fix Shape: Convert (32, 32, 3) → (3, 32, 32)\n    images = np.transpose(images, (0, 3, 1, 2))  # New shape: (10000, 3, 32, 32)\n    \n    # ✅ Convert data to float32\n    images = images.astype(np.float32) / 255.0  # Normalize to [0,1]\n    \n    # Convert to a PyTorch tensor\n    images = torch.tensor(images, dtype=torch.float32)\n    \n    return images\n\n# Load test images from the provided file path.\ntest_images = load_custom_test_set(\"/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl\")\nprint(\"Loaded custom test set with shape:\", test_images.shape)  # e.g., (10000, 3, 32, 32)\n\n########################################################################\n# 3) Normalize Test Images\n########################################################################\n# Use the same normalization as in training: mean=0.5, std=0.5 for each channel.\npkl_mean = [0.513, 0.497, 0.462]\npkl_std = [0.263, 0.260, 0.273]\n\nnormalize = transforms.Normalize(mean=pkl_mean, std=pkl_std)\n\ndef preprocess_images(images):\n    # Loop over each image to apply normalization\n    for i in range(images.shape[0]):\n        images[i] = normalize(images[i])\n    return images\n\ntest_images = preprocess_images(test_images)\n\n########################################################################\n# 4) Create DataLoader for Test Set\n########################################################################\ntest_dataset = torch.utils.data.TensorDataset(test_images)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None, bn_momentum=0.9):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass NarrowResNet34(nn.Module):\n    def __init__(self, num_classes=10, width_multiplier=0.75,\n                 bn_momentum=0.9, dropout=0.5):\n        \"\"\"\n        ResNet-34 with fewer channels (width_multiplier) and optional dropout.\n        \"\"\"\n        super(NarrowResNet34, self).__init__()\n        # ResNet-34 basic configuration is: [3, 4, 6, 3]\n        self.block_layers = [3, 4, 6, 3]\n\n        base_channels = [64, 128, 256, 512]\n        self.channels = [int(c * width_multiplier) for c in base_channels]\n        self.in_planes = self.channels[0]\n\n        # Initial convolution\n        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_planes, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Stages\n        self.layer1 = self._make_layer(self.channels[0], self.block_layers[0], stride=1, bn_momentum=bn_momentum)\n        self.layer2 = self._make_layer(self.channels[1], self.block_layers[1], stride=2, bn_momentum=bn_momentum)\n        self.layer3 = self._make_layer(self.channels[2], self.block_layers[2], stride=2, bn_momentum=bn_momentum)\n        self.layer4 = self._make_layer(self.channels[3], self.block_layers[3], stride=2, bn_momentum=bn_momentum)\n\n        # Global average pooling + dropout + linear\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=dropout)\n        self.fc = nn.Linear(self.channels[3] * BasicBlock.expansion, num_classes)\n\n        self._init_weights()\n\n    def _make_layer(self, planes, blocks, stride=1, bn_momentum=0.9):\n        downsample = None\n        if stride != 1 or self.in_planes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes, momentum=bn_momentum),\n            )\n\n        layers = []\n        layers.append(BasicBlock(self.in_planes, planes, stride, downsample, bn_momentum=bn_momentum))\n        self.in_planes = planes\n        for _ in range(1, blocks):\n            layers.append(BasicBlock(self.in_planes, planes, bn_momentum=bn_momentum))\n\n        return nn.Sequential(*layers)\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.dropout(x)  # Dropout before final FC\n        x = self.fc(x)\n        return x\n\n\n\n#def create_model():\n    #return WideResNet(depth=28, widen_factor=2, num_classes=10, dropout=0.0)\n\nmodel = NarrowResNet34(width_multiplier=0.75,num_classes=10,bn_momentum=0.9, dropout=0.5).to(device)\n\n########################################################################\n# 6) Load the Saved Model Weights\n########################################################################\nimport os\nweights_path = \"/kaggle/working/resnet18_v1_state.pth\"  # adjust this if your weights file has a different name\nif os.path.exists(weights_path):\n    model.load_state_dict(torch.load(weights_path, map_location=device))\n    print(\"Loaded model weights from\", weights_path)\nelse:\n    print(\"Weights file not found. Using the current model in memory.\")\n\nmodel.eval()\n\n########################################################################\n# 7) Run Inference on the Test Set\n########################################################################\nall_predictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = batch[0].to(device)\n        outputs = model(inputs)\n        _, predicted = outputs.max(1)\n        all_predictions.extend(predicted.cpu().numpy())\n\n########################################################################\n# 8) Create Submission CSV File\n########################################################################\nsubmission = pd.DataFrame({\n    'ID': np.arange(len(all_predictions)),\n    'Labels': all_predictions\n})\nsubmission.to_csv('/kaggle/working/submission_v1.csv', index=False)\nprint(\"Submission file saved as submission_v1.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}